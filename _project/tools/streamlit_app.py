import streamlit as st
import json
import pandas as pd
import os
import re
import sqlite3
from collections import defaultdict
from fpdf import FPDF
from docx import Document
from gpt_handler import analyze_long_text as ana
from rule_engine import apply_rules_to_chart, load_rules, analyze_with_rules
from visualizer import show_visualizations

# === ì„¤ì • ===
st.set_page_config(page_title="í†µí•© ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“Š í†µí•© ë¶„ì„ê¸°")
st.caption("AI + ê·œì¹™ ê¸°ë°˜ ë¶„ì„ + í‚¤ì›Œë“œ ì¶”ì¶œ + PDF ë¦¬í¬íŠ¸")

# === ìœ í‹¸ í•¨ìˆ˜ ===
def extract_text(file):
    suffix = file.name.split(".")[-1]
    if suffix == "docx":
        doc = Document(file)
        return "\n".join(p.text for p in doc.paragraphs)
    else:
        return file.read().decode("utf-8")

def extract_rules_advanced(text):
    pattern = re.compile(r'(.+?)\s*(ê²½ìš°|ì¼ ê²½ìš°|ì´ë¼ë©´|ì´ë©´|ì¼ ë•Œ|ì¸ ê²½ìš°|í–ˆì„ ë•Œ|í•˜ê²Œ ë˜ë©´|í•œë‹¤ë©´|í•  ê²½ìš°|ì‹œ|ë•Œ)\s*[:,]?\s*(.+?)(?:\.|$)')
    rules = {}
    for match in pattern.finditer(text):
        condition = match.group(1).strip()
        result = match.group(3).strip()
        condition_clean = re.sub(r"(ì€ëŠ”ì´ê°€|ì„ë¥¼|ì—|ì—ì„œ|ì˜|ë¡œ|ìœ¼ë¡œ|ë„|ë§Œ|ê³¼|ì™€|ì´ë©°|ì´ë‚˜|í•˜ê³ |ë¶€í„°|ê¹Œì§€)$", "", condition)
        result_clean = re.sub(r"(ì€ëŠ”ì´ê°€|ì„ë¥¼|ì—|ì—ì„œ|ì˜|ë¡œ|ìœ¼ë¡œ|ë„|ë§Œ|ê³¼|ì™€|ì´ë©°|ì´ë‚˜|í•˜ê³ |ë¶€í„°|ê¹Œì§€)$", "", result)
        rules.setdefault(condition_clean, []).append(result_clean)
    return rules

def extract_sections(text):
    keywords = ['í˜¼ì¸', 'ê²°í˜¼', 'å†å©š', 'åˆå©š', 'ë°°ìš°ìê¶', 'ë¶€ê¶', 'å¤«æ˜Ÿ', 'å¦»å®®', 'å‰¯å¤«å®®']
    lines = text.splitlines()
    results, current_block, capture = [], [], False
    for line in lines:
        if any(kw in line for kw in keywords):
            capture = True
        if capture:
            if line.strip():
                current_block.append(line.strip())
            else:
                if current_block:
                    results.append("\n".join(current_block))
                    current_block, capture = [], False
    return results, keywords

def analyze_keywords(results, keywords):
    keyword_hits = defaultdict(list)
    for text in results:
        for kw in keywords:
            if kw in text:
                keyword_hits[kw].append(text)
    df = {"í‚¤ì›Œë“œ": [], "ë¹ˆë„": [], "ì˜ˆì‹œ ì¼ë¶€": []}
    for k, v in keyword_hits.items():
        df["í‚¤ì›Œë“œ"].append(k)
        df["ë¹ˆë„"].append(len(v))
        df["ì˜ˆì‹œ ì¼ë¶€"].append(v[0][:100] + "..." if v else "")
    return pd.DataFrame(df), keyword_hits

def generate_pdf_report(filename, rule_data: dict, keyword_summary_df):
    pdf = FPDF()
    pdf.add_page()
    font_path = "NanumGothic.ttf"
    pdf.add_font("Nanum", "", font_path, uni=True)
    pdf.add_font("Nanum", "B", font_path, uni=True)
    pdf.set_font("Nanum", 'B', 16)
    pdf.cell(200, 10, txt="ì‚¬ì£¼ ë¬¸ì„œ ë¶„ì„ ë³´ê³ ì„œ", ln=True, align="C")
    pdf.ln(10)
    pdf.set_font("Nanum", 'B', 12)
    pdf.cell(200, 10, txt="ì¶”ì¶œëœ í•´ì„ ê·œì¹™", ln=True)
    for k, v_list in rule_data.items():
        pdf.set_font("Nanum", 'B', 11)
        pdf.cell(200, 8, txt=f"[{k}]", ln=True)
        pdf.set_font("Nanum", '', 11)
        for v in v_list:
            pdf.multi_cell(0, 6, f"- {v}")
    pdf.add_page()
    pdf.set_font("Nanum", 'B', 12)
    pdf.cell(200, 10, txt="í˜¼ì¸ í‚¤ì›Œë“œ ë¶„ì„ ìš”ì•½", ln=True)
    for i, row in keyword_summary_df.iterrows():
        pdf.set_font("Nanum", '', 11)
        line = f"{row['í‚¤ì›Œë“œ']} ({row['ë¹ˆë„']}íšŒ): {row['ì˜ˆì‹œ ì¼ë¶€']}"
        pdf.multi_cell(0, 6, line)
    pdf.output(filename)

# === íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ ===
uploaded_file = st.file_uploader("ğŸ“‚ íŒŒì¼ ì—…ë¡œë“œ (json, txt, docx)", type=["json", "txt", "docx"])
if uploaded_file:
    file_ext = uploaded_file.name.split(".")[-1]
    if file_ext == "json":
        saju_data = json.load(uploaded_file)
        text = json.dumps(saju_data, ensure_ascii=False, indent=2)
    else:
        text = extract_text(uploaded_file)

    st.success("âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
    st.text_area("ğŸ“‘ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°", text, height=300)

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("### ğŸ¤– GPT ë¶„ì„")
        if st.button("GPTë¡œ ë¶„ì„"):
            with st.spinner("GPT ë¶„ì„ ì¤‘..."):
                result = ana(text)
                st.text_area("ğŸ“˜ ë¶„ì„ ê²°ê³¼", result, height=300)

    with col2:
        st.markdown("### ğŸ“œ ê·œì¹™ ê¸°ë°˜ ì¶”ì¶œ")
        if st.button("ê·œì¹™ ë¶„ì„ ì‹¤í–‰"):
            with st.spinner("ê·œì¹™ ë¶„ì„ ì¤‘..."):
                rules = extract_rules_advanced(text)
                st.json(rules)

    with col3:
        st.markdown("### ğŸ” í‚¤ì›Œë“œ ë¶„ì„")
        if st.button("í˜¼ì¸ í‚¤ì›Œë“œ ë¶„ì„"):
            with st.spinner("í˜¼ì¸ í‚¤ì›Œë“œ ë¶„ì„ ì¤‘..."):
                blocks, keywords = extract_sections(text)
                df, keyword_map = analyze_keywords(blocks, keywords)
                st.dataframe(df)

    if st.button("ğŸ“„ PDF ë³´ê³ ì„œ ìƒì„±"):
        rules = extract_rules_advanced(text)
        blocks, keywords = extract_sections(text)
        df, keyword_map = analyze_keywords(blocks, keywords)
        os.makedirs("output", exist_ok=True)
        pdf_path = "output/ì‚¬ì£¼_ë³´ê³ ì„œ.pdf"
        generate_pdf_report(pdf_path, rules, df)
        st.download_button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ", open(pdf_path, "rb"), file_name="ì‚¬ì£¼_ë³´ê³ ì„œ.pdf")
